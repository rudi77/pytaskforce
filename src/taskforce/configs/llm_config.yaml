# LLM Service Configuration
# Provider-agnostic configuration using LiteLLM model string prefixes.
#
# The provider is determined by the model string prefix:
#   - No prefix       → OpenAI          (needs OPENAI_API_KEY)
#   - "anthropic/"    → Anthropic        (needs ANTHROPIC_API_KEY)
#   - "gemini/"       → Google Gemini    (needs GEMINI_API_KEY)
#   - "azure/"        → Azure OpenAI     (needs AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION)
#   - "ollama/"       → Ollama (local)   (no key needed)
#   - "openai/"       → OpenAI-compat    (needs OPENAI_API_KEY + api_base in model_params)
#
# See https://docs.litellm.ai/docs/providers for the full list.

default_model: "powerful-1"

# Model alias mapping: short name → LiteLLM model string
models:
  main: "azure/gpt-4.1"
  fast: "azure/gpt-5-mini"
  powerful: "azure/gpt-4.1"
  powerful-1: "azure/gpt-5-mini"
  legacy: "azure/gpt-4.1"

  # ── Anthropic Claude models ─────────────────────────────────────────
  claude-opus: "anthropic/claude-opus-4-6"
  claude-sonnet: "anthropic/claude-sonnet-4-6"
  claude-haiku: "anthropic/claude-haiku-4-5-20251001"

  # ── Multi-provider examples (uncomment to use) ──────────────────────
  # gemini-pro: "gemini/gemini-2.5-pro"
  # azure-main: "azure/my-gpt4-deployment"
  # local-llama: "ollama/llama3"
  # kimi: "openai/moonshot-v1-8k"   # set api_base in model_params below

# Per-model default parameters
# LiteLLM's drop_params=True ensures unsupported params are ignored per provider
model_params:
  gpt-4:
    temperature: 0.7
    top_p: 1.0
    max_tokens: 2000
    frequency_penalty: 0.0
    presence_penalty: 0.0

  gpt-4.1:
    temperature: 0.2
    top_p: 1.0
    max_tokens: 2000

  gpt-4.1-mini:
    temperature: 0.7
    max_tokens: 10000

  # ── GPT-5 family ──────────────────────────────────────────────────
  # GPT-5 models do NOT support `temperature` — use `reasoning_effort`
  # ("low", "medium", "high") instead. LiteLLM's drop_params=True
  # silently strips any unsupported params, but setting them correctly
  # avoids relying on that safety net.
  gpt-5:
    reasoning_effort: "medium"
    max_tokens: 4000

  gpt-5-mini:
    reasoning_effort: "low"
    max_tokens: 10000

  # ── Anthropic Claude models ────────────────────────────────────────
  anthropic/claude-opus-4-6:
    max_tokens: 16384
    temperature: 0.7

  anthropic/claude-sonnet-4-6:
    max_tokens: 8192
    temperature: 0.7

  anthropic/claude-haiku-4-5-20251001:
    max_tokens: 4096
    temperature: 0.7

  # ── Examples for other providers ────────────────────────────────────
  #
  # gemini/gemini-2.5-pro:
  #   max_tokens: 4096
  #   temperature: 0.7
  #
  # openai/moonshot-v1-8k:       # Kimi / OpenAI-compatible
  #   api_base: "https://api.moonshot.cn/v1"
  #   max_tokens: 4096

# Fallback parameters when no model-specific config matches
default_params:
  temperature: 0.7
  max_tokens: 2000
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0

# Retry policy for transient API errors
retry:
  max_attempts: 3
  backoff_multiplier: 2
  timeout: 60

logging:
  log_prompts: false
  log_completions: false
  log_token_usage: true
  log_latency: true
  log_parameter_mapping: true

tracing:
  enabled: true
  # Mode: 'file', 'phoenix', or 'both'
  mode: "file"
  file_config:
    path: "traces/llm_traces.jsonl"
  phoenix_config:
    collector_endpoint: "http://localhost:6006/v1/traces"
    grpc_endpoint: "http://localhost:4317"
    project_name: "taskforce"

# ── Dynamic LLM Routing (uncomment to enable) ──────────────────────
# Routes different agent phases to different models automatically.
# Planning strategies emit phase hints that the LLMRouter matches
# against these rules. First matching rule wins.
#
# IMPORTANT: Do not use the following names as model aliases above,
# as they are reserved phase hints and will bypass routing rules:
#   planning, reasoning, acting, reflecting, summarizing
#
# Phase hints emitted by planning strategies:
#   planning    — plan generation / task decomposition
#   reasoning   — ReAct main loop (tool selection + reasoning)
#   acting      — plan step execution (PlanAndExecute / SPAR act)
#   reflecting  — SPAR reflect phase / self-critique
#   summarizing — final answer synthesis
#
routing:
  enabled: true
  default_model: main
  rules:
    - condition: "hint:planning"
      model: powerful
    - condition: "hint:reasoning"
      model: fast          # <--- DAS WURDE GEÄNDERT (vorher: powerful)
    - condition: "hint:reflecting"
      model: main          # <--- (vorher: powerful)
    - condition: "hint:acting"
      model: main
    - condition: "hint:summarizing"
      model: fast

# ============================================================================
# Environment Variable Quick Reference
# ============================================================================
#
# OpenAI:
#   export OPENAI_API_KEY="sk-..."
#
# Anthropic:
#   export ANTHROPIC_API_KEY="sk-ant-..."
#
# Google Gemini:
#   export GEMINI_API_KEY="..."
#
# Azure OpenAI (LiteLLM convention):
#   export AZURE_API_KEY="..."
#   export AZURE_API_BASE="https://your-resource.openai.azure.com/"
#   export AZURE_API_VERSION="2024-02-15-preview"
#
# Alternative (Microsoft convention) - auto-mapped by taskforce:
#   export AZURE_OPENAI_API_KEY="..."
#   export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
#   export AZURE_OPENAI_API_VERSION="2024-02-15-preview"
#
# Ollama (local):
#   No key needed – just run `ollama serve`
#
# OpenAI-compatible (e.g., Kimi, GLM):
#   export OPENAI_API_KEY="your-provider-key"
#   Set api_base in model_params above
# ============================================================================
