# LLM Service Configuration
# Provider-agnostic configuration using LiteLLM model string prefixes.
#
# The provider is determined by the model string prefix:
#   - No prefix       → OpenAI          (needs OPENAI_API_KEY)
#   - "anthropic/"    → Anthropic        (needs ANTHROPIC_API_KEY)
#   - "gemini/"       → Google Gemini    (needs GEMINI_API_KEY)
#   - "azure/"        → Azure OpenAI     (needs AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION)
#   - "ollama/"       → Ollama (local)   (no key needed)
#   - "openai/"       → OpenAI-compat    (needs OPENAI_API_KEY + api_base in model_params)
#
# See https://docs.litellm.ai/docs/providers for the full list.

default_model: "powerful-1"

# Model alias mapping: short name → LiteLLM model string
models:
  main: "azure/gpt-4.1"
  fast: "azure/gpt-4.1"
  powerful: "azure/gpt-4.1"
  powerful-1: "azure/gpt-5.0-mini"
  legacy: "azure/gpt-4.1"

  # ── Multi-provider examples (uncomment to use) ──────────────────────
  # claude-sonnet: "anthropic/claude-sonnet-4-20250514"
  # gemini-pro: "gemini/gemini-2.5-pro"
  # azure-main: "azure/my-gpt4-deployment"
  # local-llama: "ollama/llama3"
  # kimi: "openai/moonshot-v1-8k"   # set api_base in model_params below

# Per-model default parameters
# LiteLLM's drop_params=True ensures unsupported params are ignored per provider
model_params:
  gpt-4:
    temperature: 0.7
    top_p: 1.0
    max_tokens: 2000
    frequency_penalty: 0.0
    presence_penalty: 0.0

  gpt-4.1:
    temperature: 0.2
    top_p: 1.0
    max_tokens: 2000

  gpt-4.1-mini:
    temperature: 0.7
    max_tokens: 10000

  gpt-5:
    effort: "medium"
    reasoning: "balanced"
    max_tokens: 4000

  gpt-5-mini:
    effort: "low"
    reasoning: "minimal"
    #max_tokens: 10000

  # ── Examples for other providers ────────────────────────────────────
  # anthropic/claude-sonnet-4-20250514:
  #   max_tokens: 8192
  #   temperature: 0.7
  #
  # gemini/gemini-2.5-pro:
  #   max_tokens: 4096
  #   temperature: 0.7
  #
  # openai/moonshot-v1-8k:       # Kimi / OpenAI-compatible
  #   api_base: "https://api.moonshot.cn/v1"
  #   max_tokens: 4096

# Fallback parameters when no model-specific config matches
default_params:
  temperature: 0.7
  max_tokens: 2000
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0

# Retry policy for transient API errors
retry:
  max_attempts: 3
  backoff_multiplier: 2
  timeout: 60

logging:
  log_prompts: false
  log_completions: false
  log_token_usage: true
  log_latency: true
  log_parameter_mapping: true

tracing:
  enabled: true
  # Mode: 'file', 'phoenix', or 'both'
  mode: "file"
  file_config:
    path: "traces/llm_traces.jsonl"
  phoenix_config:
    collector_endpoint: "http://localhost:6006/v1/traces"
    grpc_endpoint: "http://localhost:4317"
    project_name: "taskforce"

# ============================================================================
# Environment Variable Quick Reference
# ============================================================================
#
# OpenAI:
#   export OPENAI_API_KEY="sk-..."
#
# Anthropic:
#   export ANTHROPIC_API_KEY="sk-ant-..."
#
# Google Gemini:
#   export GEMINI_API_KEY="..."
#
# Azure OpenAI:
#   export AZURE_API_KEY="..."
#   export AZURE_API_BASE="https://your-resource.openai.azure.com/"
#   export AZURE_API_VERSION="2024-02-15-preview"
#
# Ollama (local):
#   No key needed – just run `ollama serve`
#
# OpenAI-compatible (e.g., Kimi, GLM):
#   export OPENAI_API_KEY="your-provider-key"
#   Set api_base in model_params above
# ============================================================================
