# LLM Configuration for Accounting Agent
# Uses OpenAI models as primary provider.
#
# Requires: OPENAI_API_KEY environment variable
# See https://docs.litellm.ai/docs/providers/openai for details.

default_model: "main"

# Model alias mapping: short name → LiteLLM model string
# - main: standard model for tool-call decisions and final responses
# - fast: cheaper/faster model for simple orchestration steps (summarizing, etc.)
# - powerful: strong reasoning model for complex planning and reflection
models:
  main: "azure/gpt-4.1"
  fast: "azure/gpt-5.0-mini"
  powerful: "azure/gpt-4.1"
  powerful-1: "azure/gpt-5.0-mini"

# Per-model default parameters
# LiteLLM's drop_params=True ensures unsupported params are ignored per provider
model_params:
  gpt-4.1:
    temperature: 0.2
    max_tokens: 8192

  gpt-5.0-mini:
    temperature: 0.7
    max_tokens: 4096

# Fallback parameters when no model-specific config matches
default_params:
  temperature: 0.7
  max_tokens: 4096

# Retry policy for transient API errors
retry:
  max_attempts: 3
  backoff_multiplier: 2
  timeout: 60

logging:
  log_prompts: false
  log_completions: false
  log_token_usage: true
  log_latency: true
  log_parameter_mapping: true

# LLM Router: dynamic per-call model selection
# Rules are evaluated in order; first match wins.
# Phase hints are emitted by planning strategies as the `model` parameter:
#   - "reasoning"   → main ReAct reasoning loop
#   - "summarizing" → final response / compression summaries
#   - "planning"    → plan generation phase
#   - "reflecting"  → SPAR reflect phase
# Tool-presence rules fire when the LLM is given tools (acting) vs not (summarizing).
routing:
  enabled: true
  default_model: main
  rules:
    # Summarizing final responses and context compression use fast model
    - condition: "hint:summarizing"
      model: fast
    # Planning and reflection use powerful model for quality
    - condition: "hint:planning"
      model: powerful
    - condition: "hint:reflecting"
      model: powerful
    # When tools are provided → standard reasoning → main model
    - condition: has_tools
      model: main
    # When no tools are provided (pure text generation) → fast model
    - condition: no_tools
      model: fast

tracing:
  enabled: true
  mode: "file"
  file_config:
    path: "traces/llm_traces.jsonl"
